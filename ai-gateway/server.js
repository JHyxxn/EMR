/**
 * AI Gateway ì„œë²„
 * 
 * ë‹´ë‹¹ìž: ê¹€ì§€í˜„ (AI Gateway)
 * 
 * ì£¼ìš” ê¸°ëŠ¥:
 * - ë‹¤ì¤‘ AI ëª¨ë¸ í†µí•© (OpenAI, Anthropic, Google)
 * - ìžë™ í´ë°± ë©”ì»¤ë‹ˆì¦˜ (ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ìžë™ ì „í™˜)
 * - Rate Limit ê´€ë¦¬
 * - ìž„ìƒë…¸íŠ¸ ìš”ì•½ (SOAP ë¶„ì„)
 * - Lab/ë°”ì´íƒˆ ìš”ì•½
 * - ì¦ìƒ ë¶„ì„ AI (ì§„ë‹¨ ì¶”ì²œ)
 * - ì²˜ë°© ê°€ì´ë“œ (ì•½ë¬¼ ìƒí˜¸ìž‘ìš© ë° ìš©ëŸ‰ ê°€ì´ë“œ)
 * - ê²€ì‚¬ ê²°ê³¼ AI ë¶„ì„
 * 
 * ê¸°ìˆ  ìŠ¤íƒ:
 * - Node.js + Express.js
 * - OpenAI API
 * - Anthropic Claude API
 * - Google Gemini API
 * - Rate Limit ê´€ë¦¬
 * - CORS ì„¤ì •
 * 
 * AI ëª¨ë¸:
 * - OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
 * - Anthropic: claude-3-5-sonnet, claude-3-haiku, claude-3-opus
 * - Google: gemini-1.5-pro, gemini-1.5-flash, gemini-1.0-pro
 * 
 * API ì—”ë“œí¬ì¸íŠ¸:
 * - /health - í—¬ìŠ¤ì²´í¬
 * - /insight/clinical-note - ìž„ìƒë…¸íŠ¸ ìš”ì•½
 * - /insight/lab-summary - Lab ìš”ì•½
 * - /insight/symptom-analysis - ì¦ìƒ ë¶„ì„
 * - /insight/prescription-guide - ì²˜ë°© ê°€ì´ë“œ
 * - /insight/test-analysis - ê²€ì‚¬ ê²°ê³¼ ë¶„ì„
 * - /models/status - ëª¨ë¸ ìƒíƒœ í™•ì¸
 */
// ai-gateway/server.js
import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';

dotenv.config();

const app = express();
app.use(cors({
    origin: ['http://localhost:3000', 'http://localhost:5173', 'http://localhost:5174', 'http://localhost:5175', 'http://127.0.0.1:3000', 'http://127.0.0.1:5173', 'http://127.0.0.1:5174', 'http://127.0.0.1:5175'],
    credentials: true
}));
app.use(express.json());

const PORT = Number(process.env.PORT ?? 5001);

// Rate limit ê´€ë¦¬
const rateLimitMap = new Map();
const RATE_LIMIT_WINDOW = 60 * 1000; // 1ë¶„
const RATE_LIMIT_MAX_REQUESTS = 30; // 1ë¶„ë‹¹ ìµœëŒ€ 30ê°œ ìš”ì²­ (ì¦ê°€)

// Rate limit ì²´í¬ í•¨ìˆ˜
function checkRateLimit(provider) {
    const now = Date.now();
    const key = `${provider}_${Math.floor(now / RATE_LIMIT_WINDOW)}`;
    
    if (!rateLimitMap.has(key)) {
        rateLimitMap.set(key, 0);
    }
    
    const currentCount = rateLimitMap.get(key);
    if (currentCount >= RATE_LIMIT_MAX_REQUESTS) {
        return false;
    }
    
    rateLimitMap.set(key, currentCount + 1);
    return true;
}

// Rate limit ëŒ€ê¸° í•¨ìˆ˜
function waitForRateLimit(provider) {
    return new Promise(resolve => {
        const checkInterval = setInterval(() => {
            if (checkRateLimit(provider)) {
                clearInterval(checkInterval);
                resolve();
            }
        }, 1000); // 1ì´ˆë§ˆë‹¤ ì²´í¬
    });
}

/** -------------------------
 *  ê³µí†µ: ë‹¤ì¤‘ AI ëª¨ë¸ ì§€ì› LLM í˜¸ì¶œ í•¨ìˆ˜
 *  ------------------------- */

// AI ëª¨ë¸ ì„¤ì •
const AI_MODELS = {
    openai: {
        name: 'OpenAI',
        baseUrl: 'https://api.openai.com/v1/chat/completions',
        apiKey: process.env.OPENAI_API_KEY,
        defaultModel: 'gpt-4o-mini',
        models: ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo']
    },
    anthropic: {
        name: 'Anthropic Claude',
        baseUrl: 'https://api.anthropic.com/v1/messages',
        apiKey: process.env.ANTHROPIC_API_KEY,
        defaultModel: 'claude-3-haiku-20240307',
        models: ['claude-3-5-sonnet-20241022', 'claude-3-haiku-20240307', 'claude-3-opus-20240229']
    },
    google: {
        name: 'Google Gemini',
        baseUrl: 'https://generativelanguage.googleapis.com/v1beta/models',
        apiKey: process.env.GOOGLE_API_KEY,
        defaultModel: 'gemini-1.5-flash',
        models: ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.0-pro']
    }
};

// OpenAI í˜¸ì¶œ í•¨ìˆ˜
async function callOpenAI({ system, prompt, model = AI_MODELS.openai.defaultModel }) {
    const r = await fetch(AI_MODELS.openai.baseUrl, {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${AI_MODELS.openai.apiKey}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            model,
            messages: [
                { role: 'system', content: system },
                { role: 'user', content: prompt }
            ],
            temperature: 0.3
        })
    });

    const data = await r.json();
    if (!r.ok) throw new Error(data?.error?.message || 'OpenAI_ERROR');
    return data.choices?.[0]?.message?.content ?? '';
}

// Anthropic Claude í˜¸ì¶œ í•¨ìˆ˜
async function callAnthropic({ system, prompt, model = AI_MODELS.anthropic.defaultModel }) {
    const r = await fetch(AI_MODELS.anthropic.baseUrl, {
        method: 'POST',
        headers: {
            'x-api-key': AI_MODELS.anthropic.apiKey,
            'Content-Type': 'application/json',
            'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
            model,
            max_tokens: 1000,
            messages: [
                { role: 'user', content: `${system}\n\n${prompt}` }
            ]
        })
    });

    const data = await r.json();
    if (!r.ok) throw new Error(data?.error?.message || 'Anthropic_ERROR');
    return data.content?.[0]?.text ?? '';
}

// Google Gemini í˜¸ì¶œ í•¨ìˆ˜
async function callGoogle({ system, prompt, model = AI_MODELS.google.defaultModel }) {
    const url = `${AI_MODELS.google.baseUrl}/${model}:generateContent?key=${AI_MODELS.google.apiKey}`;
    
    const r = await fetch(url, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            contents: [{
                parts: [{
                    text: `${system}\n\n${prompt}`
                }]
            }],
            generationConfig: {
                temperature: 0.3,
                maxOutputTokens: 1000
            }
        })
    });

    const data = await r.json();
    if (!r.ok) throw new Error(data?.error?.message || 'Google_ERROR');
    return data.candidates?.[0]?.content?.parts?.[0]?.text ?? '';
}

// í†µí•© LLM í˜¸ì¶œ í•¨ìˆ˜ (ìžë™ í´ë°± ì§€ì›)
async function callLLM({ system, prompt, provider = 'auto', model = null }) {
    const providers = provider === 'auto' 
        ? ['openai', 'anthropic', 'google'] 
        : [provider];

    let lastError = null;

    for (const providerName of providers) {
        const config = AI_MODELS[providerName];
        if (!config || !config.apiKey) {
            console.warn(`Provider ${providerName} not configured, skipping...`);
            continue;
        }

        // Rate limit ì²´í¬
        if (!checkRateLimit(providerName)) {
            console.warn(`Rate limit exceeded for ${providerName}, waiting...`);
            await waitForRateLimit(providerName);
        }

        try {
            const selectedModel = model || config.defaultModel;
            console.log(`Trying ${config.name} with model ${selectedModel}...`);

            let result;
            switch (providerName) {
                case 'openai':
                    result = await callOpenAI({ system, prompt, model: selectedModel });
                    break;
                case 'anthropic':
                    result = await callAnthropic({ system, prompt, model: selectedModel });
                    break;
                case 'google':
                    result = await callGoogle({ system, prompt, model: selectedModel });
                    break;
                default:
                    throw new Error(`Unsupported provider: ${providerName}`);
            }

            console.log(`âœ… Success with ${config.name}`);
            return { result, provider: providerName, model: selectedModel };
        } catch (error) {
            const errorMessage = error.message || 'Unknown error';
            const isRateLimit = errorMessage.includes('rate limit') || errorMessage.includes('429') || errorMessage.includes('Rate limit reached');
            const isAuthError = errorMessage.includes('401') || errorMessage.includes('unauthorized');
            
            if (isRateLimit) {
                console.warn(`âŒ ${config.name} rate limit exceeded, waiting...`);
                await waitForRateLimit(providerName);
                continue; // ìž¬ì‹œë„
            } else if (isAuthError) {
                console.error(`âŒ ${config.name} authentication failed:`, errorMessage);
                continue; // ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ë¡œ ë„˜ì–´ê°
            } else {
                console.warn(`âŒ ${config.name} failed:`, errorMessage);
            }
            
            lastError = error;
            continue;
        }
    }

    throw new Error(`All AI providers failed. Last error: ${lastError?.message || 'Unknown error'}`);
}

/** -------------------------
 *  ê³µí†µ: ìž„ê³„ì¹˜ ê³„ì‚°(ë£°)
 *  ------------------------- */
function calcFlagsForObservation({ codeLoinc, value }) {
    const flags = [];
    const v = Number(value);
    if (codeLoinc === 'BP-SYS' && !Number.isNaN(v) && v >= 140) flags.push('HIGH_BP_SYSTOLIC');
    if (codeLoinc === 'BP-DIA' && !Number.isNaN(v) && v >= 90)  flags.push('HIGH_BP_DIASTOLIC');
    if (codeLoinc === 'GLU-FBS' && !Number.isNaN(v) && v >= 200) flags.push('HIGH_GLUCOSE');
    return flags;
}

/** -------------------------
 *  Health
 *  ------------------------- */
app.get('/health', (_req, res) => {
    res.json({ ok: true, service: 'ai-gateway' });
});

/** -------------------------
 *  ìž„ìƒë…¸íŠ¸ ìš”ì•½
 *  - ?provider=rule | auto | openai | anthropic | google
 *  - auto: ìžë™ í´ë°± (openai -> anthropic -> google -> rule)
 *  ------------------------- */
app.post('/insight/clinical-note', async (req, res) => {
    try {
        const provider = (req.query.provider || 'auto').toLowerCase();
        const { text = '', patient = {} } = req.body ?? {};
        const name = patient?.name ?? 'í™˜ìž';
        const short = text.length > 400 ? text.slice(0, 400) + 'â€¦' : text;

        // AI ëª¨ë¸ ì‚¬ìš© ì‹œë„
        if (provider !== 'rule') {
            try {
                const systemPrompt = 'ë‹¹ì‹ ì€ ê²½í—˜ ë§Žì€ ì˜ì‚¬ìž…ë‹ˆë‹¤. SOAP ì§„ë£Œ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ê°„ê²°í•˜ê³  ì •í™•í•œ ì˜í•™ì  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”. ì˜í•™ì  ë‹¨ì •ì€ í”¼í•˜ê³ , êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ ì‚¬í•­ì„ ì œì‹œí•˜ì„¸ìš”.';
                const userPrompt = `ë‹¤ìŒ SOAP ì§„ë£Œ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ì˜í•™ì  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

í™˜ìž ì •ë³´: ${name} ${patient?.age ? `${patient.age}ì„¸` : ''} ${patient?.sex || ''}
SOAP ê¸°ë¡: """${short}"""

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

í™˜ìž: ${name} (${patient?.age || ''}ì„¸, ${patient?.sex || ''})

í•µì‹¬ ìš”ì•½:
â€¢ [ì£¼ìš” ì¦ìƒë§Œ ê°„ë‹¨ížˆ]
â€¢ [ê¸°ì¡´ ì§ˆí™˜ë§Œ ê°„ë‹¨ížˆ]

ì¶”ì²œ ì²˜ë°©:
â€¢ [êµ¬ì²´ì ì¸ ì•½ë¬¼ëª…] [ìš©ëŸ‰] ([ë³µìš©ë²•])
â€¢ [êµ¬ì²´ì ì¸ ì•½ë¬¼ëª…] [ìš©ëŸ‰] ([ë³µìš©ë²•])

ì¶”ì²œ ê²€ì‚¬:
â€¢ [êµ¬ì²´ì ì¸ ê²€ì‚¬ëª…]
â€¢ [êµ¬ì²´ì ì¸ ê²€ì‚¬ëª…]

ìœ„ í˜•ì‹ ì™¸ì˜ ë‹¤ë¥¸ ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.`;

                const aiResult = await callLLM({
                    system: systemPrompt,
                    prompt: userPrompt,
                    provider: provider === 'auto' ? 'auto' : provider
                });

                return res.json({ 
                    summary: aiResult.result, 
                    provider: aiResult.provider, 
                    model: aiResult.model,
                    highlights: [] 
                });
            } catch (e) {
                console.warn('AI models failed. fallback to rule:', e.message);
                // í´ë°± ì•„ëž˜ë¡œ ê³„ì† ì§„í–‰
            }
        }

        // fallback: rule (êµ¬ì¡°í™”ëœ í˜•ì‹)
        // SOAP ë‚´ìš©ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì¶”ì²œ ìƒì„±
        let symptoms = [];
        let prescriptions = [];
        let tests = [];
        
        // ì¦ìƒ ì¶”ì¶œ
        if (short.includes('ë‘í†µ') || short.includes('ì–´ì§€ëŸ¼ì¦') || short.includes('êµ¬í† ')) {
            symptoms.push('ë‘í†µ, ì–´ì§€ëŸ¼ì¦, êµ¬í† ');
            prescriptions.push('ì•„ì„¸íŠ¸ì•„ë¯¸ë…¸íŽœ 500mg (1T tid)');
            prescriptions.push('ì´ë¶€í”„ë¡œíŽœ 400mg (1T bid)');
            tests.push('ë‡ŒCT');
            tests.push('í˜ˆì•¡ê²€ì‚¬ (CBC, CRP)');
        } else if (short.includes('ë³µí†µ') || short.includes('ë³µë¶€')) {
            symptoms.push('ë³µí†µ');
            prescriptions.push('ì œì‚°ì œ (1T tid)');
            prescriptions.push('ì§„ê²½ì œ (1T bid)');
            tests.push('ë³µë¶€ì´ˆìŒíŒŒ');
            tests.push('í˜ˆì•¡ê²€ì‚¬ (CBC, CRP)');
        } else if (short.includes('ê°ê¸°') || short.includes('ê¸°ì¹¨') || short.includes('ì½§ë¬¼')) {
            symptoms.push('ê°ê¸° ì¦ìƒ');
            prescriptions.push('í•´ì—´ì§„í†µì œ (1T tid)');
            prescriptions.push('ê¸°ì¹¨ì•½ (1T tid)');
            tests.push('í˜ˆì•¡ê²€ì‚¬ (CBC)');
        } else {
            symptoms.push('ì¼ë°˜ ì¦ìƒ');
            prescriptions.push('ì§„í†µì œ (1T tid)');
            tests.push('í˜ˆì•¡ê²€ì‚¬ (CBC)');
        }
        
        const summary = `í™˜ìž: ${name} (${patient?.age || ''}ì„¸, ${patient?.sex || ''})
í•µì‹¬ ìš”ì•½: ${symptoms.join(', ')}
ì¶”ì²œ ì²˜ë°©: ${prescriptions.join(', ')}
ì¶”ì²œ ê²€ì‚¬: ${tests.join(', ')}`;
        res.json({ summary, provider: 'rule', highlights: [] });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  Lab/ë°”ì´íƒˆ ìš”ì•½ (ë£° ê¸°ë°˜)
 *  ------------------------- */
app.post('/insight/lab-summary', async (req, res) => {
    try {
        const { observations = [] } = req.body ?? {};
        const flagged = observations
            .map(o => ({ ...o, flags: calcFlagsForObservation(o) }))
            .filter(o => o.flags.length > 0);

        let summary = 'ìµœê·¼ ê´€ì°°ì¹˜ ìš”ì•½:';
        if (flagged.length === 0) {
            summary += ' íŠ¹ì´ì‚¬í•­ ì—†ìŒ.';
        } else {
            for (const f of flagged.slice(0, 5)) {
                summary += `\n- ${f.codeLoinc}=${f.value}${f.unit ? f.unit : ''} (${f.flags.join(',')})`;
            }
            summary += '\nì£¼ì˜: ìˆ˜ì¹˜ê°€ ìž„ê³„ ë²”ìœ„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìž„ìƒì  íŒë‹¨ì„ ìœ„í•´ ì¶”ê°€ í™•ì¸ í•„ìš”.';
        }

        res.json({ summary, provider: 'rule', flaggedCount: flagged.length });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  ì¦ìƒ ë¶„ì„ AI (í™˜ìž ì¦ìƒ ê¸°ë°˜ ì§„ë‹¨ ì¶”ì²œ)
 *  - ?provider=auto | openai | anthropic | google
 *  ------------------------- */
app.post('/insight/symptom-analysis', async (req, res) => {
    try {
        const provider = (req.query.provider || 'auto').toLowerCase();
        const { symptoms = [], patient = {}, observations = [] } = req.body ?? {};
        const name = patient?.name ?? 'í™˜ìž';
        const age = patient?.age;
        const sex = patient?.sex;

        // AI ëª¨ë¸ ì‚¬ìš© ì‹œë„
        if (provider !== 'rule') {
            try {
                const systemPrompt = `ë‹¹ì‹ ì€ ê²½í—˜ ë§Žì€ ì˜ë£Œì§„ìž…ë‹ˆë‹¤. í™˜ìžì˜ ì¦ìƒê³¼ ê´€ì°°ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§„ë‹¨ì„ ì¶”ì²œí•˜ë˜, ì˜í•™ì  ë‹¨ì •ì€ ê¸ˆì§€í•©ë‹ˆë‹¤. 
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ê³ , ë‹¤ìŒ í˜•ì‹ì„ ì •í™•ížˆ ë”°ë¼ì£¼ì„¸ìš”:

## 1. ì£¼ìš” ì¦ìƒ ìš”ì•½
- í™˜ìžì˜ ì£¼ìš” ì¦ìƒì„ ê°„ë‹¨ížˆ ìš”ì•½

## 2. ê°€ëŠ¥í•œ ì§„ë‹¨ (í™•ë¥  ìˆœ)
ê° ì§„ë‹¨ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œ:
- **ì§„ë‹¨ëª…**: í™•ë¥  (ì˜ˆ: 70%)
- ê°„ë‹¨í•œ ì„¤ëª…

## 3. ì¶”ê°€ ê²€ì‚¬ ê¶Œìž¥ì‚¬í•­
- í•„ìš”í•œ ê²€ì‚¬ ëª©ë¡
- ê° ê²€ì‚¬ì˜ ëª©ì 

## 4. ì£¼ì˜ì‚¬í•­
- ì‘ê¸‰ìƒí™© ì§•í›„
- ì¦‰ì‹œ ì˜ë£Œì§„ ìƒë‹´ì´ í•„ìš”í•œ ê²½ìš°
- ì¼ë°˜ì ì¸ ì£¼ì˜ì‚¬í•­

âš ï¸ ì¤‘ìš”: ëª¨ë“  ì§„ë‹¨ì€ ì°¸ê³ ìš©ì´ë©°, ìµœì¢… ì§„ë‹¨ì€ ì˜ë£Œì§„ì´ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.`;

                const userPrompt = `í™˜ìž ì •ë³´:
- ì´ë¦„: ${name} ${age ? `${age}ì„¸` : ''} ${sex || ''}
- ì¦ìƒ: ${symptoms.join(', ')}
- ê´€ì°°ì¹˜: ${observations.map(o => `${o.codeLoinc}=${o.value}${o.unit || ''}`).join(', ')}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ ì§„ë‹¨ ì¶”ì²œì„ í•´ì£¼ì„¸ìš”.`;

                const aiResult = await callLLM({
                    system: systemPrompt,
                    prompt: userPrompt,
                    provider: provider === 'auto' ? 'auto' : provider
                });

                return res.json({ 
                    analysis: aiResult.result, 
                    provider: aiResult.provider, 
                    model: aiResult.model,
                    symptoms: symptoms,
                    observations: observations
                });
            } catch (e) {
                console.warn('AI models failed. fallback to rule:', e.message);
                // í´ë°± ì•„ëž˜ë¡œ ê³„ì† ì§„í–‰
            }
        }

        // fallback: rule
        const analysis = `${name} ì¦ìƒ ë¶„ì„:
- ì£¼ìš” ì¦ìƒ: ${symptoms.join(', ')}
- ê´€ì°°ì¹˜: ${observations.length}ê°œ í•­ëª© í™•ì¸
- ê¶Œìž¥ì‚¬í•­: ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ ì¶”ê°€ ê²€ì‚¬ ë° ì „ë¬¸ì˜ ìƒë‹´ í•„ìš”
- ì£¼ì˜: AI ë¶„ì„ì€ ì°¸ê³ ìš©ì´ë©°, ìµœì¢… ì§„ë‹¨ì€ ì˜ë£Œì§„ì´ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.`;

        res.json({ 
            analysis, 
            provider: 'rule', 
            symptoms: symptoms,
            observations: observations
        });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  ì²˜ë°© ê°€ì´ë“œ (ì•½ë¬¼ ìƒí˜¸ìž‘ìš© ë° ìš©ëŸ‰ ê°€ì´ë“œ)
 *  - ?provider=auto | openai | anthropic | google
 *  ------------------------- */
app.post('/insight/prescription-guide', async (req, res) => {
    try {
        const provider = (req.query.provider || 'auto').toLowerCase();
        const { medications = [], patient = {}, currentMedications = [] } = req.body ?? {};
        const name = patient?.name ?? 'í™˜ìž';
        const age = patient?.age;
        const weight = patient?.weight;

        // AI ëª¨ë¸ ì‚¬ìš© ì‹œë„
        if (provider !== 'rule') {
            try {
                const systemPrompt = `ë‹¹ì‹ ì€ ìž„ìƒì•½ì‚¬ìž…ë‹ˆë‹¤. í™˜ìžì˜ ì•½ë¬¼ ì²˜ë°©ì— ëŒ€í•œ ìƒí˜¸ìž‘ìš©ê³¼ ìš©ëŸ‰ ê°€ì´ë“œë¥¼ ì œê³µí•˜ë˜, ì˜í•™ì  ë‹¨ì •ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ê³ , ë‹¤ìŒ í˜•ì‹ì„ ì •í™•ížˆ ë”°ë¼ì£¼ì„¸ìš”:

## 1. ì•½ë¬¼ ìƒí˜¸ìž‘ìš© ë¶„ì„
- ì²˜ë°© ì•½ë¬¼ ê°„ ìƒí˜¸ìž‘ìš©
- ê¸°ì¡´ ì•½ë¬¼ê³¼ì˜ ìƒí˜¸ìž‘ìš©
- ìœ„í—˜ë„ ìˆ˜ì¤€ (ë‚®ìŒ/ë³´í†µ/ë†’ìŒ)

## 2. ìš©ëŸ‰ ê°€ì´ë“œ
- ê° ì•½ë¬¼ì˜ ê¶Œìž¥ ìš©ëŸ‰
- í™˜ìž íŠ¹ì„± ê³ ë ¤ì‚¬í•­ (ë‚˜ì´, ì²´ì¤‘ ë“±)
- ë³µìš©ë²• ë° ì‹œê¸°

## 3. ì£¼ì˜ì‚¬í•­
- ë¶€ìž‘ìš© ëª¨ë‹ˆí„°ë§
- ê¸ˆê¸°ì‚¬í•­
- íŠ¹ë³„ ì£¼ì˜ê°€ í•„ìš”í•œ ê²½ìš°

## 4. ëª¨ë‹ˆí„°ë§ ê¶Œìž¥ì‚¬í•­
- ì •ê¸° ê²€ì‚¬ í•­ëª©
- ë¶€ìž‘ìš© ì§•í›„
- ì¶”ì  ê´€ì°° ì‚¬í•­

âš ï¸ ì¤‘ìš”: ëª¨ë“  ê°€ì´ë“œëŠ” ì°¸ê³ ìš©ì´ë©°, ìµœì¢… ì²˜ë°©ì€ ì˜ë£Œì§„ì´ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.`;

                const userPrompt = `í™˜ìž ì •ë³´:
- ì´ë¦„: ${name} ${age ? `${age}ì„¸` : ''} ${weight ? `${weight}kg` : ''}
- ì²˜ë°© ì•½ë¬¼: ${medications.join(', ')}
- í˜„ìž¬ ë³µìš© ì¤‘ì¸ ì•½ë¬¼: ${currentMedications.join(', ')}

ì•½ë¬¼ ìƒí˜¸ìž‘ìš©ê³¼ ìš©ëŸ‰ ê°€ì´ë“œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.`;

                const aiResult = await callLLM({
                    system: systemPrompt,
                    prompt: userPrompt,
                    provider: provider === 'auto' ? 'auto' : provider
                });

                return res.json({ 
                    guide: aiResult.result, 
                    provider: aiResult.provider, 
                    model: aiResult.model,
                    medications: medications,
                    currentMedications: currentMedications
                });
            } catch (e) {
                console.warn('AI models failed. fallback to rule:', e.message);
                // í´ë°± ì•„ëž˜ë¡œ ê³„ì† ì§„í–‰
            }
        }

        // fallback: rule
        const guide = `${name} ì²˜ë°© ê°€ì´ë“œ:
- ì²˜ë°© ì•½ë¬¼: ${medications.join(', ')}
- í˜„ìž¬ ë³µìš© ì•½ë¬¼: ${currentMedications.join(', ')}
- ê¶Œìž¥ì‚¬í•­: ì•½ë¬¼ ìƒí˜¸ìž‘ìš© í™•ì¸ì„ ìœ„í•´ ì•½ì‚¬ ìƒë‹´ í•„ìš”
- ì£¼ì˜: AI ê°€ì´ë“œëŠ” ì°¸ê³ ìš©ì´ë©°, ìµœì¢… ì²˜ë°©ì€ ì˜ë£Œì§„ì´ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.`;

        res.json({ 
            guide, 
            provider: 'rule', 
            medications: medications,
            currentMedications: currentMedications
        });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  ê²€ì‚¬ ê²°ê³¼ AI ë¶„ì„
 *  ------------------------- */
app.post('/insight/test-analysis', async (req, res) => {
    try {
        const provider = (req.query.provider || 'auto').toLowerCase();
        const { testResult = {}, patient = {} } = req.body ?? {};
        const name = patient?.name ?? 'í™˜ìž';
        
        // AI ëª¨ë¸ ì‚¬ìš© ì‹œë„
        if (provider !== 'rule') {
            try {
                const systemPrompt = 'ë‹¹ì‹ ì€ ê²½í—˜ ë§Žì€ ê²€ì‚¬ì‹¤ ì˜ì‚¬ìž…ë‹ˆë‹¤. ê²€ì‚¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì˜í•™ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•´ì„ì„ ì œê³µí•˜ì„¸ìš”.';
                const userPrompt = `ë‹¤ìŒ ê²€ì‚¬ ê²°ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

í™˜ìž: ${name} (${patient?.age || ''}ì„¸, ${patient?.sex || ''})
ê²€ì‚¬ëª…: ${testResult.testName || 'ê²€ì‚¬'}
ê²°ê³¼ê°’: ${testResult.value || ''} ${testResult.unit || ''}
ì •ìƒë²”ìœ„: ${testResult.referenceRange || ''}
ê²°ê³¼ìƒíƒœ: ${testResult.status || ''}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

ê²€ì‚¬ ê²°ê³¼ ë¶„ì„:
â€¢ [ê²°ê³¼ í•´ì„]
â€¢ [ìž„ìƒì  ì˜ë¯¸]
â€¢ [ì¶”ê°€ ê²€ì‚¬ ê¶Œê³ ì‚¬í•­]
â€¢ [ì£¼ì˜ì‚¬í•­]`;

                const aiResult = await callLLM({ system: systemPrompt, prompt: userPrompt, provider });
                
                res.json({ 
                    analysis: aiResult.result, 
                    provider: aiResult.provider, 
                    model: aiResult.model
                });
                return;
            } catch (e) {
                console.warn('AI models failed. fallback to rule:', e.message);
            }
        }

        // fallback: rule ê¸°ë°˜ ë¶„ì„
        const { testName, value, unit, status, referenceRange } = testResult;
        
        let analysis = `ê²€ì‚¬ ê²°ê³¼ ë¶„ì„:\n`;
        
        if (status === 'critical') {
            analysis += `â€¢ âš ï¸ ìœ„í—˜: ${testName} ê²°ê³¼ê°€ ìœ„í—˜ ìˆ˜ì¤€ìž…ë‹ˆë‹¤.\n`;
            analysis += `â€¢ ì¦‰ì‹œ ì˜ë£Œì§„ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\n`;
            analysis += `â€¢ ì¶”ê°€ ê²€ì‚¬ë‚˜ ì‘ê¸‰ ì²˜ì¹˜ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.\n`;
            analysis += `â€¢ í™˜ìž ìƒíƒœë¥¼ ë©´ë°€ížˆ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.`;
        } else if (status === 'abnormal') {
            analysis += `â€¢ âš ï¸ ì£¼ì˜: ${testName} ê²°ê³¼ê°€ ì •ìƒ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.\n`;
            analysis += `â€¢ ìž„ìƒì  ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ ì¶”ê°€ ê²€ì‚¬ë¥¼ ê¶Œê³ í•©ë‹ˆë‹¤.\n`;
            analysis += `â€¢ í™˜ìž ì¦ìƒê³¼ í•¨ê»˜ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.\n`;
            analysis += `â€¢ ì¶”í›„ ìž¬ê²€ì‚¬ë¥¼ í†µí•´ ë³€í™”ë¥¼ ê´€ì°°í•˜ì„¸ìš”.`;
        } else {
            analysis += `â€¢ âœ… ì •ìƒ: ${testName} ê²°ê³¼ê°€ ì •ìƒ ë²”ìœ„ ë‚´ì— ìžˆìŠµë‹ˆë‹¤.\n`;
            analysis += `â€¢ íŠ¹ì´ì‚¬í•­ì´ ì—†ìœ¼ë©° í˜„ìž¬ ìƒíƒœëŠ” ì–‘í˜¸í•©ë‹ˆë‹¤.\n`;
            analysis += `â€¢ ì •ê¸°ì ì¸ ê²€ì‚¬ë¥¼ í†µí•´ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.\n`;
            analysis += `â€¢ í™˜ìžì—ê²Œ ì•ˆì‹¬ì‹œì¼œ ë“œë¦¬ì„¸ìš”.`;
        }

        res.json({ 
            analysis, 
            provider: 'rule'
        });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  AI ëª¨ë¸ ìƒíƒœ í™•ì¸
 *  ------------------------- */
app.get('/models/status', async (_req, res) => {
    try {
        const status = {};
        
        for (const [provider, config] of Object.entries(AI_MODELS)) {
            status[provider] = {
                name: config.name,
                configured: !!config.apiKey,
                models: config.models,
                defaultModel: config.defaultModel
            };
        }

        res.json({ 
            providers: status,
            timestamp: new Date().toISOString()
        });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

app.listen(PORT, '127.0.0.1', () => {
    console.log(`ðŸ§  AI Gateway running http://localhost:${PORT}`)
}).on('error', (err) => {
    console.error('AI Gateway listen error:', err)
});