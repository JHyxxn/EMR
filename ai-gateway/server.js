// ai-gateway/server.js
import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';

dotenv.config();

const app = express();
app.use(cors());
app.use(express.json());

const PORT = Number(process.env.PORT ?? 5001);

/** -------------------------
 *  ê³µí†µ: ë‹¤ì¤‘ AI ëª¨ë¸ ì§€ì› LLM í˜¸ì¶œ í•¨ìˆ˜
 *  ------------------------- */

// AI ëª¨ë¸ ì„¤ì •
const AI_MODELS = {
    openai: {
        name: 'OpenAI',
        baseUrl: 'https://api.openai.com/v1/chat/completions',
        apiKey: process.env.OPENAI_API_KEY,
        defaultModel: 'gpt-4o-mini',
        models: ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo']
    },
    anthropic: {
        name: 'Anthropic Claude',
        baseUrl: 'https://api.anthropic.com/v1/messages',
        apiKey: process.env.ANTHROPIC_API_KEY,
        defaultModel: 'claude-3-haiku-20240307',
        models: ['claude-3-5-sonnet-20241022', 'claude-3-haiku-20240307', 'claude-3-opus-20240229']
    },
    google: {
        name: 'Google Gemini',
        baseUrl: 'https://generativelanguage.googleapis.com/v1beta/models',
        apiKey: process.env.GOOGLE_API_KEY,
        defaultModel: 'gemini-1.5-flash',
        models: ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.0-pro']
    }
};

// OpenAI í˜¸ì¶œ í•¨ìˆ˜
async function callOpenAI({ system, prompt, model = AI_MODELS.openai.defaultModel }) {
    const r = await fetch(AI_MODELS.openai.baseUrl, {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${AI_MODELS.openai.apiKey}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            model,
            messages: [
                { role: 'system', content: system },
                { role: 'user', content: prompt }
            ],
            temperature: 0.3
        })
    });

    const data = await r.json();
    if (!r.ok) throw new Error(data?.error?.message || 'OpenAI_ERROR');
    return data.choices?.[0]?.message?.content ?? '';
}

// Anthropic Claude í˜¸ì¶œ í•¨ìˆ˜
async function callAnthropic({ system, prompt, model = AI_MODELS.anthropic.defaultModel }) {
    const r = await fetch(AI_MODELS.anthropic.baseUrl, {
        method: 'POST',
        headers: {
            'x-api-key': AI_MODELS.anthropic.apiKey,
            'Content-Type': 'application/json',
            'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
            model,
            max_tokens: 1000,
            messages: [
                { role: 'user', content: `${system}\n\n${prompt}` }
            ]
        })
    });

    const data = await r.json();
    if (!r.ok) throw new Error(data?.error?.message || 'Anthropic_ERROR');
    return data.content?.[0]?.text ?? '';
}

// Google Gemini í˜¸ì¶œ í•¨ìˆ˜
async function callGoogle({ system, prompt, model = AI_MODELS.google.defaultModel }) {
    const url = `${AI_MODELS.google.baseUrl}/${model}:generateContent?key=${AI_MODELS.google.apiKey}`;
    
    const r = await fetch(url, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            contents: [{
                parts: [{
                    text: `${system}\n\n${prompt}`
                }]
            }],
            generationConfig: {
                temperature: 0.3,
                maxOutputTokens: 1000
            }
        })
    });

    const data = await r.json();
    if (!r.ok) throw new Error(data?.error?.message || 'Google_ERROR');
    return data.candidates?.[0]?.content?.parts?.[0]?.text ?? '';
}

// í†µí•© LLM í˜¸ì¶œ í•¨ìˆ˜ (ìžë™ í´ë°± ì§€ì›)
async function callLLM({ system, prompt, provider = 'auto', model = null }) {
    const providers = provider === 'auto' 
        ? ['openai', 'anthropic', 'google'] 
        : [provider];

    let lastError = null;

    for (const providerName of providers) {
        const config = AI_MODELS[providerName];
        if (!config || !config.apiKey) {
            console.warn(`Provider ${providerName} not configured, skipping...`);
            continue;
        }

        try {
            const selectedModel = model || config.defaultModel;
            console.log(`Trying ${config.name} with model ${selectedModel}...`);

            let result;
            switch (providerName) {
                case 'openai':
                    result = await callOpenAI({ system, prompt, model: selectedModel });
                    break;
                case 'anthropic':
                    result = await callAnthropic({ system, prompt, model: selectedModel });
                    break;
                case 'google':
                    result = await callGoogle({ system, prompt, model: selectedModel });
                    break;
                default:
                    throw new Error(`Unsupported provider: ${providerName}`);
            }

            console.log(`âœ… Success with ${config.name}`);
            return { result, provider: providerName, model: selectedModel };
        } catch (error) {
            console.warn(`âŒ ${config.name} failed:`, error.message);
            lastError = error;
            continue;
        }
    }

    throw new Error(`All AI providers failed. Last error: ${lastError?.message || 'Unknown error'}`);
}

/** -------------------------
 *  ê³µí†µ: ìž„ê³„ì¹˜ ê³„ì‚°(ë£°)
 *  ------------------------- */
function calcFlagsForObservation({ codeLoinc, value }) {
    const flags = [];
    const v = Number(value);
    if (codeLoinc === 'BP-SYS' && !Number.isNaN(v) && v >= 140) flags.push('HIGH_BP_SYSTOLIC');
    if (codeLoinc === 'BP-DIA' && !Number.isNaN(v) && v >= 90)  flags.push('HIGH_BP_DIASTOLIC');
    if (codeLoinc === 'GLU-FBS' && !Number.isNaN(v) && v >= 200) flags.push('HIGH_GLUCOSE');
    return flags;
}

/** -------------------------
 *  Health
 *  ------------------------- */
app.get('/health', (_req, res) => {
    res.json({ ok: true, service: 'ai-gateway' });
});

/** -------------------------
 *  ìž„ìƒë…¸íŠ¸ ìš”ì•½
 *  - ?provider=rule | auto | openai | anthropic | google
 *  - auto: ìžë™ í´ë°± (openai -> anthropic -> google -> rule)
 *  ------------------------- */
app.post('/insight/clinical-note', async (req, res) => {
    try {
        const provider = (req.query.provider || 'auto').toLowerCase();
        const { text = '', patient = {} } = req.body ?? {};
        const name = patient?.name ?? 'í™˜ìž';
        const short = text.length > 400 ? text.slice(0, 400) + 'â€¦' : text;

        // AI ëª¨ë¸ ì‚¬ìš© ì‹œë„
        if (provider !== 'rule') {
            try {
                const systemPrompt = 'ì˜í•™ì  ë‹¨ì • ê¸ˆì§€. ê´€ì°° ê¸°ë°˜ìœ¼ë¡œ 2~4ë¬¸ìž¥ ìš”ì•½. í•œêµ­ì–´.';
                const userPrompt = `ë‹¤ìŒ ë¬¸ì§„ ë‚´ìš©ì„ 2~3ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ìœ„í—˜ ì‹ í˜¸ê°€ ìžˆìœ¼ë©´ ë§ˆì§€ë§‰ì— í•œ ë¬¸ìž¥ìœ¼ë¡œ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì œì•ˆí•´ì¤˜.
- í™˜ìž: ${name} ${patient?.age ? `${patient.age}ì„¸` : ''} ${patient?.sex || ''}
- ë¬¸ì§„: """${short}"""`;

                const aiResult = await callLLM({
                    system: systemPrompt,
                    prompt: userPrompt,
                    provider: provider === 'auto' ? 'auto' : provider
                });

                return res.json({ 
                    summary: aiResult.result, 
                    provider: aiResult.provider, 
                    model: aiResult.model,
                    highlights: [] 
                });
            } catch (e) {
                console.warn('AI models failed. fallback to rule:', e.message);
                // í´ë°± ì•„ëž˜ë¡œ ê³„ì† ì§„í–‰
            }
        }

        // fallback: rule
        const summary =
            `${name} ë¬¸ì§„ ìš”ì•½: ${short}\n` +
            `- ì¦ìƒ ìš”ì•½ê³¼ ê²½ê³¼ë¥¼ ê°„ë‹¨ížˆ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ìœ„ê¸‰ ì§•í›„ ë°œê²¬ ì‹œ ë‹´ë‹¹ìžì™€ ìƒì˜í•˜ì„¸ìš”.`;
        res.json({ summary, provider: 'rule', highlights: [] });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  Lab/ë°”ì´íƒˆ ìš”ì•½ (ë£° ê¸°ë°˜)
 *  ------------------------- */
app.post('/insight/lab-summary', async (req, res) => {
    try {
        const { observations = [] } = req.body ?? {};
        const flagged = observations
            .map(o => ({ ...o, flags: calcFlagsForObservation(o) }))
            .filter(o => o.flags.length > 0);

        let summary = 'ìµœê·¼ ê´€ì°°ì¹˜ ìš”ì•½:';
        if (flagged.length === 0) {
            summary += ' íŠ¹ì´ì‚¬í•­ ì—†ìŒ.';
        } else {
            for (const f of flagged.slice(0, 5)) {
                summary += `\n- ${f.codeLoinc}=${f.value}${f.unit ? f.unit : ''} (${f.flags.join(',')})`;
            }
            summary += '\nì£¼ì˜: ìˆ˜ì¹˜ê°€ ìž„ê³„ ë²”ìœ„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìž„ìƒì  íŒë‹¨ì„ ìœ„í•´ ì¶”ê°€ í™•ì¸ í•„ìš”.';
        }

        res.json({ summary, provider: 'rule', flaggedCount: flagged.length });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  ì¦ìƒ ë¶„ì„ AI (í™˜ìž ì¦ìƒ ê¸°ë°˜ ì§„ë‹¨ ì¶”ì²œ)
 *  - ?provider=auto | openai | anthropic | google
 *  ------------------------- */
app.post('/insight/symptom-analysis', async (req, res) => {
    try {
        const provider = (req.query.provider || 'auto').toLowerCase();
        const { symptoms = [], patient = {}, observations = [] } = req.body ?? {};
        const name = patient?.name ?? 'í™˜ìž';
        const age = patient?.age;
        const sex = patient?.sex;

        // AI ëª¨ë¸ ì‚¬ìš© ì‹œë„
        if (provider !== 'rule') {
            try {
                const systemPrompt = `ë‹¹ì‹ ì€ ê²½í—˜ ë§Žì€ ì˜ë£Œì§„ìž…ë‹ˆë‹¤. í™˜ìžì˜ ì¦ìƒê³¼ ê´€ì°°ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§„ë‹¨ì„ ì¶”ì²œí•˜ë˜, ì˜í•™ì  ë‹¨ì •ì€ ê¸ˆì§€í•©ë‹ˆë‹¤. 
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ê³ , ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì£¼ìš” ì¦ìƒ ìš”ì•½
2. ê°€ëŠ¥í•œ ì§„ë‹¨ (í™•ë¥  ìˆœ)
3. ì¶”ê°€ ê²€ì‚¬ ê¶Œìž¥ì‚¬í•­
4. ì£¼ì˜ì‚¬í•­`;

                const userPrompt = `í™˜ìž ì •ë³´:
- ì´ë¦„: ${name} ${age ? `${age}ì„¸` : ''} ${sex || ''}
- ì¦ìƒ: ${symptoms.join(', ')}
- ê´€ì°°ì¹˜: ${observations.map(o => `${o.codeLoinc}=${o.value}${o.unit || ''}`).join(', ')}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§„ë‹¨ ì¶”ì²œì„ í•´ì£¼ì„¸ìš”.`;

                const aiResult = await callLLM({
                    system: systemPrompt,
                    prompt: userPrompt,
                    provider: provider === 'auto' ? 'auto' : provider
                });

                return res.json({ 
                    analysis: aiResult.result, 
                    provider: aiResult.provider, 
                    model: aiResult.model,
                    symptoms: symptoms,
                    observations: observations
                });
            } catch (e) {
                console.warn('AI models failed. fallback to rule:', e.message);
                // í´ë°± ì•„ëž˜ë¡œ ê³„ì† ì§„í–‰
            }
        }

        // fallback: rule
        const analysis = `${name} ì¦ìƒ ë¶„ì„:
- ì£¼ìš” ì¦ìƒ: ${symptoms.join(', ')}
- ê´€ì°°ì¹˜: ${observations.length}ê°œ í•­ëª© í™•ì¸
- ê¶Œìž¥ì‚¬í•­: ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ ì¶”ê°€ ê²€ì‚¬ ë° ì „ë¬¸ì˜ ìƒë‹´ í•„ìš”
- ì£¼ì˜: AI ë¶„ì„ì€ ì°¸ê³ ìš©ì´ë©°, ìµœì¢… ì§„ë‹¨ì€ ì˜ë£Œì§„ì´ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.`;

        res.json({ 
            analysis, 
            provider: 'rule', 
            symptoms: symptoms,
            observations: observations
        });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  ì²˜ë°© ê°€ì´ë“œ (ì•½ë¬¼ ìƒí˜¸ìž‘ìš© ë° ìš©ëŸ‰ ê°€ì´ë“œ)
 *  - ?provider=auto | openai | anthropic | google
 *  ------------------------- */
app.post('/insight/prescription-guide', async (req, res) => {
    try {
        const provider = (req.query.provider || 'auto').toLowerCase();
        const { medications = [], patient = {}, currentMedications = [] } = req.body ?? {};
        const name = patient?.name ?? 'í™˜ìž';
        const age = patient?.age;
        const weight = patient?.weight;

        // AI ëª¨ë¸ ì‚¬ìš© ì‹œë„
        if (provider !== 'rule') {
            try {
                const systemPrompt = `ë‹¹ì‹ ì€ ìž„ìƒì•½ì‚¬ìž…ë‹ˆë‹¤. í™˜ìžì˜ ì•½ë¬¼ ì²˜ë°©ì— ëŒ€í•œ ìƒí˜¸ìž‘ìš©ê³¼ ìš©ëŸ‰ ê°€ì´ë“œë¥¼ ì œê³µí•˜ë˜, ì˜í•™ì  ë‹¨ì •ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ê³ , ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì•½ë¬¼ ìƒí˜¸ìž‘ìš© ë¶„ì„
2. ìš©ëŸ‰ ê°€ì´ë“œ
3. ì£¼ì˜ì‚¬í•­
4. ëª¨ë‹ˆí„°ë§ ê¶Œìž¥ì‚¬í•­`;

                const userPrompt = `í™˜ìž ì •ë³´:
- ì´ë¦„: ${name} ${age ? `${age}ì„¸` : ''} ${weight ? `${weight}kg` : ''}
- ì²˜ë°© ì•½ë¬¼: ${medications.join(', ')}
- í˜„ìž¬ ë³µìš© ì¤‘ì¸ ì•½ë¬¼: ${currentMedications.join(', ')}

ì•½ë¬¼ ìƒí˜¸ìž‘ìš©ê³¼ ìš©ëŸ‰ ê°€ì´ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.`;

                const aiResult = await callLLM({
                    system: systemPrompt,
                    prompt: userPrompt,
                    provider: provider === 'auto' ? 'auto' : provider
                });

                return res.json({ 
                    guide: aiResult.result, 
                    provider: aiResult.provider, 
                    model: aiResult.model,
                    medications: medications,
                    currentMedications: currentMedications
                });
            } catch (e) {
                console.warn('AI models failed. fallback to rule:', e.message);
                // í´ë°± ì•„ëž˜ë¡œ ê³„ì† ì§„í–‰
            }
        }

        // fallback: rule
        const guide = `${name} ì²˜ë°© ê°€ì´ë“œ:
- ì²˜ë°© ì•½ë¬¼: ${medications.join(', ')}
- í˜„ìž¬ ë³µìš© ì•½ë¬¼: ${currentMedications.join(', ')}
- ê¶Œìž¥ì‚¬í•­: ì•½ë¬¼ ìƒí˜¸ìž‘ìš© í™•ì¸ì„ ìœ„í•´ ì•½ì‚¬ ìƒë‹´ í•„ìš”
- ì£¼ì˜: AI ê°€ì´ë“œëŠ” ì°¸ê³ ìš©ì´ë©°, ìµœì¢… ì²˜ë°©ì€ ì˜ë£Œì§„ì´ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.`;

        res.json({ 
            guide, 
            provider: 'rule', 
            medications: medications,
            currentMedications: currentMedications
        });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

/** -------------------------
 *  AI ëª¨ë¸ ìƒíƒœ í™•ì¸
 *  ------------------------- */
app.get('/models/status', async (_req, res) => {
    try {
        const status = {};
        
        for (const [provider, config] of Object.entries(AI_MODELS)) {
            status[provider] = {
                name: config.name,
                configured: !!config.apiKey,
                models: config.models,
                defaultModel: config.defaultModel
            };
        }

        res.json({ 
            providers: status,
            timestamp: new Date().toISOString()
        });
    } catch (e) {
        console.error(e);
        res.status(500).json({ error: 'server_error' });
    }
});

app.listen(PORT, '127.0.0.1', () => {
    console.log(`ðŸ§  AI Gateway running http://localhost:${PORT}`)
}).on('error', (err) => {
    console.error('AI Gateway listen error:', err)
});